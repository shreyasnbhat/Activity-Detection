{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL: Convolution Autoencoder -> LSTM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Statements and Global Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from keras import backend as K\n",
    "from keras.layers import Conv2D, Dropout, LSTM, BatchNormalization, Input,Activation, MaxPooling2D, Flatten, Dense,TimeDistributed, UpSampling2D\n",
    "from keras.models import Model, load_model\n",
    "from keras import metrics \n",
    "import random\n",
    "import pickle\n",
    "\n",
    "# Directory paths\n",
    "VIDEOS_DIR = './Videos/'\n",
    "IMAGES_DIR = './Images/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Labels for classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all classes\n",
    "classes = ['Kicking', 'Riding-Horse', 'Running', 'SkateBoarding', \n",
    "           'Swing-Bench', 'Lifting', 'Swing-Side', 'Walking', 'Golf-Swing']\n",
    "\n",
    "# Create dictionary to label each class\n",
    "class_to_index = {}\n",
    "for i in range(len(classes)):\n",
    "    class_to_index[classes[i]] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standard Function Definitions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute(X,Y):\n",
    "    train_size = X.shape[0]\n",
    "    permutation_train = np.random.permutation(train_size)\n",
    "    X = X[permutation_train]\n",
    "    Y = Y[permutation_train]\n",
    "    return X,Y\n",
    "\n",
    "def load_image(path,image_size):\n",
    "    image = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, image_size)\n",
    "    return image\n",
    "\n",
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)]\n",
    "    return Y\n",
    "\n",
    "def pad(X_cnn,max_len):\n",
    "    features_len = np.shape(X_cnn)[1]\n",
    "    length = np.shape(X_cnn)[0]\n",
    "    X_cnn = list(X_cnn)\n",
    "    pad_arr = [0 for i in range(features_len)]\n",
    "    for i in range(max_len-length):\n",
    "        X_cnn.append(pad_arr)\n",
    "    return np.array(X_cnn)\n",
    "\n",
    "def evaluate(X_test,Y_test,model):\n",
    "    count = 0\n",
    "    for i in range(len(X_test)):\n",
    "        pred = model.predict(X_test[i])\n",
    "        max_pred = [np.argmax(i) for i in pred]\n",
    "        counts = np.bincount(max_pred)\n",
    "        class_pred = np.argmax(counts)\n",
    "        #class_pred = max_pred\n",
    "        actual = np.argmax(Y_test[i])\n",
    "        #print(\"Max Preds time\", max_pred)\n",
    "        #print(\"Pred\",classes[class_pred],\"Actual\",classes[actual])\n",
    "        #print()\n",
    "        if class_pred == actual:\n",
    "            count += 1\n",
    "    return float(count)/float(len(Y_test)) * 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of directories in Videos folder\n",
    "videos = []\n",
    "for x in classes:\n",
    "    videos.append(list(os.listdir(VIDEOS_DIR+x+'/')))\n",
    "videos\n",
    "\n",
    "def build_dataset(image_size, images_per_video = 10):\n",
    "    # Initialise image vectors\n",
    "    X_train_images = []\n",
    "    Y_train_images = []\n",
    "    \n",
    "    # For each class in classes, do:\n",
    "    vid_num, l_vid_num = 0, []\n",
    "    for i in range(len(classes)):\n",
    "        # Name of class\n",
    "        cls = classes[i]\n",
    "        \n",
    "        print (\"Processing \", cls, \" videos...\")\n",
    "        # For each dir in corresponding videos folder, do:\n",
    "        for j in range(len(videos[i])):\n",
    "            # Get dir no, eg. '012', '016', etc.\n",
    "            vid = videos[i][j]\n",
    "            print (\"Going through \", vid, \"...\")\n",
    "            \n",
    "            # Append to new paths\n",
    "            video_r = VIDEOS_DIR+cls+'/'+ vid +'/'\n",
    "            image_r = IMAGES_DIR+cls+'/'+ vid +'/'\n",
    "            \n",
    "            # List all images in the given image directory\n",
    "            filelist = list(os.listdir(image_r))\n",
    "            #print(sorted(filelist))\n",
    "            \n",
    "            # Initialise X_train\n",
    "            X_train_images_video = []\n",
    "            \n",
    "            # For image in imagelist, do:\n",
    "            for fichier in filelist:\n",
    "                # Check if file is an image\n",
    "                if fichier.endswith(\".png\"):\n",
    "                    # Load and resize image\n",
    "                    image = load_image(image_r+fichier,image_size)\n",
    "                    # Append to list\n",
    "                    X_train_images_video.append(image)\n",
    "                    \n",
    "            # Randomly shuffle list\n",
    "            permutation = np.random.permutation(len(X_train_images_video))\n",
    "            X_train_images_video = np.array(X_train_images_video)[permutation]\n",
    "            \n",
    "            # If no. of images in video < images_per_video, take that\n",
    "            no_of_images = min(images_per_video, len(X_train_images_video))\n",
    "            \n",
    "            # Append to X_train images\n",
    "            X_train_images += list(X_train_images_video[:no_of_images])\n",
    "            \n",
    "            # Append to Y_train_images\n",
    "            Y_train_images += [i] * no_of_images\n",
    "            \n",
    "            # To keep track of video number\n",
    "            l_vid_num += [vid_num] * no_of_images\n",
    "            vid_num += 1\n",
    "            \n",
    "    print (\"Done.\")\n",
    "    return np.array(X_train_images), np.array(Y_train_images), l_vid_num, vid_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoder and LSTM Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder(input_dim):\n",
    "    input_img = Input(shape=input_dim)\n",
    "\n",
    "    x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "    encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    \n",
    "    x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(16, (3, 3), activation='relu')(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    encoder = Model(input_img, decoded)\n",
    "    encoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_model(encoding_dim):\n",
    "    X_input = Input(shape=encoding_dim)\n",
    "    X = LSTM(16, return_sequences=True)(X_input)\n",
    "    X = Dropout(0.3)(X)\n",
    "    X = LSTM(16, return_sequences=False)(X)\n",
    "    X = Dropout(0.3)(X)\n",
    "    X = Dense(9,activation='softmax')(X)\n",
    "    \n",
    "    lstm = Model(X_input, X)\n",
    "    lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    \n",
    "    return lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read Data and Build Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing  Kicking  videos...\n",
      "Going through  012 ...\n",
      "Going through  016 ...\n",
      "Going through  011 ...\n",
      "Going through  017 ...\n",
      "Going through  013 ...\n",
      "Going through  003 ...\n",
      "Going through  009 ...\n",
      "Going through  015 ...\n",
      "Going through  006 ...\n",
      "Going through  007 ...\n",
      "Going through  004 ...\n",
      "Going through  002 ...\n",
      "Going through  014 ...\n",
      "Going through  001 ...\n",
      "Going through  010 ...\n",
      "Going through  005 ...\n",
      "Processing  Riding-Horse  videos...\n",
      "Going through  003 ...\n",
      "Going through  009 ...\n",
      "Going through  006 ...\n",
      "Going through  007 ...\n",
      "Going through  004 ...\n",
      "Going through  002 ...\n",
      "Going through  008 ...\n",
      "Going through  001 ...\n",
      "Going through  010 ...\n",
      "Going through  005 ...\n",
      "Processing  Running  videos...\n",
      "Going through  009 ...\n",
      "Going through  006 ...\n",
      "Going through  007 ...\n",
      "Going through  004 ...\n",
      "Going through  002 ...\n",
      "Going through  008 ...\n",
      "Going through  001 ...\n",
      "Going through  010 ...\n",
      "Going through  005 ...\n",
      "Processing  SkateBoarding  videos...\n",
      "Going through  003 ...\n",
      "Going through  009 ...\n",
      "Going through  006 ...\n",
      "Going through  007 ...\n",
      "Going through  004 ...\n",
      "Going through  002 ...\n",
      "Going through  008 ...\n",
      "Going through  001 ...\n",
      "Going through  010 ...\n",
      "Going through  005 ...\n",
      "Processing  Swing-Bench  videos...\n",
      "Going through  012 ...\n",
      "Going through  016 ...\n",
      "Going through  011 ...\n",
      "Going through  017 ...\n",
      "Going through  013 ...\n",
      "Going through  003 ...\n",
      "Going through  009 ...\n",
      "Going through  015 ...\n",
      "Going through  006 ...\n",
      "Going through  007 ...\n",
      "Going through  004 ...\n",
      "Going through  002 ...\n",
      "Going through  014 ...\n",
      "Going through  008 ...\n",
      "Going through  001 ...\n",
      "Going through  010 ...\n",
      "Going through  005 ...\n",
      "Processing  Lifting  videos...\n",
      "Going through  003 ...\n",
      "Going through  004 ...\n",
      "Going through  002 ...\n",
      "Going through  001 ...\n",
      "Going through  005 ...\n",
      "Processing  Swing-Side  videos...\n",
      "Going through  011 ...\n",
      "Going through  003 ...\n",
      "Going through  009 ...\n",
      "Going through  006 ...\n",
      "Going through  007 ...\n",
      "Going through  004 ...\n",
      "Going through  002 ...\n",
      "Going through  008 ...\n",
      "Going through  001 ...\n",
      "Going through  010 ...\n",
      "Going through  005 ...\n",
      "Processing  Walking  videos...\n",
      "Going through  012 ...\n",
      "Going through  016 ...\n",
      "Going through  011 ...\n",
      "Going through  017 ...\n",
      "Going through  013 ...\n",
      "Going through  003 ...\n",
      "Going through  009 ...\n",
      "Going through  019 ...\n",
      "Going through  015 ...\n",
      "Going through  006 ...\n",
      "Going through  007 ...\n",
      "Going through  004 ...\n",
      "Going through  002 ...\n",
      "Going through  014 ...\n",
      "Going through  008 ...\n",
      "Going through  001 ...\n",
      "Going through  010 ...\n",
      "Going through  005 ...\n",
      "Going through  018 ...\n",
      "Processing  Golf-Swing  videos...\n",
      "Going through  012 ...\n",
      "Going through  011 ...\n",
      "Going through  013 ...\n",
      "Going through  003 ...\n",
      "Going through  009 ...\n",
      "Going through  006 ...\n",
      "Going through  007 ...\n",
      "Going through  004 ...\n",
      "Going through  002 ...\n",
      "Going through  014 ...\n",
      "Going through  008 ...\n",
      "Going through  001 ...\n",
      "Going through  010 ...\n",
      "Going through  005 ...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "data = build_dataset((172, 172))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_img, y_img, lvid, no_of_videos = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_16 (InputLayer)        (None, 3, 172, 172)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_63 (Conv2D)           (None, 16, 172, 172)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 16, 86, 86)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_64 (Conv2D)           (None, 8, 86, 86)         1160      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 8, 43, 43)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_65 (Conv2D)           (None, 8, 43, 43)         584       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 8, 22, 22)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_66 (Conv2D)           (None, 8, 22, 22)         584       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 8, 11, 11)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_67 (Conv2D)           (None, 8, 11, 11)         584       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_28 (UpSampling (None, 8, 22, 22)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_68 (Conv2D)           (None, 8, 22, 22)         584       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_29 (UpSampling (None, 8, 44, 44)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_69 (Conv2D)           (None, 8, 44, 44)         584       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_30 (UpSampling (None, 8, 88, 88)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_70 (Conv2D)           (None, 16, 86, 86)        1168      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_31 (UpSampling (None, 16, 172, 172)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_71 (Conv2D)           (None, 3, 172, 172)       435       \n",
      "=================================================================\n",
      "Total params: 6,131\n",
      "Trainable params: 6,131\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder = build_encoder((3, 172, 172))\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_14 (InputLayer)        (None, 10, 968)           0         \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 10, 16)            63040     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 10, 16)            0         \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 16)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 9)                 153       \n",
      "=================================================================\n",
      "Total params: 65,305\n",
      "Trainable params: 65,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm = LSTM_model((10, 968))\n",
    "lstm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1097, 172, 172, 3)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_img = x_img.astype('float32') / 255.\n",
    "x_img = np.reshape(x_img, (len(x_img), 3, 172, 172))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 877 samples, validate on 220 samples\n",
      "Epoch 1/20\n",
      "877/877 [==============================] - 144s 164ms/step - loss: 0.6704 - val_loss: 0.6413\n",
      "Epoch 2/20\n",
      "877/877 [==============================] - 141s 161ms/step - loss: 0.6329 - val_loss: 0.6594\n",
      "Epoch 3/20\n",
      "877/877 [==============================] - 126s 144ms/step - loss: 0.6238 - val_loss: 0.6234\n",
      "Epoch 4/20\n",
      "877/877 [==============================] - 128s 146ms/step - loss: 0.6164 - val_loss: 0.6233\n",
      "Epoch 5/20\n",
      "877/877 [==============================] - 175s 200ms/step - loss: 0.6142 - val_loss: 0.6303\n",
      "Epoch 6/20\n",
      "877/877 [==============================] - 147s 168ms/step - loss: 0.6136 - val_loss: 0.6177\n",
      "Epoch 7/20\n",
      "877/877 [==============================] - 151s 172ms/step - loss: 0.6126 - val_loss: 0.6264\n",
      "Epoch 8/20\n",
      "877/877 [==============================] - 167s 191ms/step - loss: 0.6102 - val_loss: 0.6172\n",
      "Epoch 9/20\n",
      "877/877 [==============================] - 160s 183ms/step - loss: 0.6100 - val_loss: 0.6153\n",
      "Epoch 10/20\n",
      "877/877 [==============================] - 150s 171ms/step - loss: 0.6080 - val_loss: 0.6162\n",
      "Epoch 11/20\n",
      "877/877 [==============================] - 160s 182ms/step - loss: 0.6057 - val_loss: 0.6079\n",
      "Epoch 12/20\n",
      "877/877 [==============================] - 140s 159ms/step - loss: 0.6029 - val_loss: 0.6132\n",
      "Epoch 13/20\n",
      "877/877 [==============================] - 125s 142ms/step - loss: 0.6027 - val_loss: 0.6065\n",
      "Epoch 14/20\n",
      "877/877 [==============================] - 132s 150ms/step - loss: 0.6026 - val_loss: 0.6100\n",
      "Epoch 15/20\n",
      "877/877 [==============================] - 151s 172ms/step - loss: 0.6010 - val_loss: 0.6040\n",
      "Epoch 16/20\n",
      "877/877 [==============================] - 137s 157ms/step - loss: 0.6000 - val_loss: 0.6040\n",
      "Epoch 17/20\n",
      "877/877 [==============================] - 147s 167ms/step - loss: 0.5996 - val_loss: 0.6043\n",
      "Epoch 18/20\n",
      "877/877 [==============================] - 130s 148ms/step - loss: 0.5989 - val_loss: 0.6036\n",
      "Epoch 19/20\n",
      "877/877 [==============================] - 125s 143ms/step - loss: 0.5984 - val_loss: 0.6031\n",
      "Epoch 20/20\n",
      "877/877 [==============================] - 122s 140ms/step - loss: 0.5978 - val_loss: 0.6035\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6882907a58>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.fit(x_img, x_img,\n",
    "                epochs=20,\n",
    "                batch_size=32,\n",
    "                shuffle=True,\n",
    "                validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_8th_layer_output = K.function([encoder.layers[0].input],\n",
    "                                  [encoder.layers[8].output])\n",
    "\n",
    "layer_output = get_8th_layer_output([x_img])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1097, 8, 11, 11)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(layer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111,)\n",
      "10\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n",
      "(10, 968)\n"
     ]
    }
   ],
   "source": [
    "lstm_in = [None] * no_of_videos\n",
    "for i in range(len(lstm_in)):\n",
    "    lstm_in[i] = []\n",
    "\n",
    "for i in range(len(layer_output)):\n",
    "    img = np.reshape(layer_output[i], -1)\n",
    "    lstm_in[lvid[i]].append(img)\n",
    "\n",
    "print (np.shape(lstm_in))\n",
    "    \n",
    "lens = [len(a) for a in lstm_in]\n",
    "max_len = max(lens)\n",
    "print (max_len)\n",
    "\n",
    "for i in range(len(lstm_in)):\n",
    "    lstm_in[i] = pad(lstm_in[i], max_len)\n",
    "    \n",
    "for a in lstm_in:\n",
    "    print (np.shape(a))\n",
    "    \n",
    "lstm_in = np.array(lstm_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111, 10, 968)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(lstm_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1097, 9)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_y = convert_to_one_hot(y_img, 9)\n",
    "np.shape(lstm_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_y = [ np.where(r==1)[0][0] for r in lstm_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " ...]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5,\n",
       "       5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "       7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "       8], dtype=object)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([None] * no_of_videos)\n",
    "    \n",
    "for i in range(len(lstm_y)):\n",
    "    y[lvid[i]] = lstm_y[i]\n",
    "    \n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(y)):\n",
    "    y[i] = int(y[i])\n",
    "\n",
    "y = np.array(y, dtype=np.int8)\n",
    "    \n",
    "y = convert_to_one_hot(y, 9)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 88 samples, validate on 23 samples\n",
      "Epoch 1/400\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 0.3502 - acc: 0.8889 - val_loss: 0.3702 - val_acc: 0.8889\n",
      "Epoch 2/400\n",
      "88/88 [==============================] - 0s 880us/step - loss: 0.3446 - acc: 0.8889 - val_loss: 0.3811 - val_acc: 0.8889\n",
      "Epoch 3/400\n",
      "88/88 [==============================] - 0s 749us/step - loss: 0.3433 - acc: 0.8889 - val_loss: 0.3874 - val_acc: 0.8889\n",
      "Epoch 4/400\n",
      "88/88 [==============================] - 0s 832us/step - loss: 0.3442 - acc: 0.8889 - val_loss: 0.3920 - val_acc: 0.8889\n",
      "Epoch 5/400\n",
      "88/88 [==============================] - 0s 975us/step - loss: 0.3424 - acc: 0.8889 - val_loss: 0.3952 - val_acc: 0.8889\n",
      "Epoch 6/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3396 - acc: 0.8889 - val_loss: 0.3987 - val_acc: 0.8889\n",
      "Epoch 7/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3418 - acc: 0.8889 - val_loss: 0.4006 - val_acc: 0.8889\n",
      "Epoch 8/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3365 - acc: 0.8889 - val_loss: 0.4022 - val_acc: 0.8889\n",
      "Epoch 9/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3374 - acc: 0.8889 - val_loss: 0.4044 - val_acc: 0.8889\n",
      "Epoch 10/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3376 - acc: 0.8889 - val_loss: 0.4067 - val_acc: 0.8889\n",
      "Epoch 11/400\n",
      "88/88 [==============================] - 0s 994us/step - loss: 0.3388 - acc: 0.8889 - val_loss: 0.4101 - val_acc: 0.8889\n",
      "Epoch 12/400\n",
      "88/88 [==============================] - 0s 996us/step - loss: 0.3346 - acc: 0.8889 - val_loss: 0.4143 - val_acc: 0.8889\n",
      "Epoch 13/400\n",
      "88/88 [==============================] - 0s 905us/step - loss: 0.3332 - acc: 0.8889 - val_loss: 0.4198 - val_acc: 0.8889\n",
      "Epoch 14/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3378 - acc: 0.8889 - val_loss: 0.4251 - val_acc: 0.8889\n",
      "Epoch 15/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3337 - acc: 0.8889 - val_loss: 0.4288 - val_acc: 0.8889\n",
      "Epoch 16/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3335 - acc: 0.8889 - val_loss: 0.4306 - val_acc: 0.8889\n",
      "Epoch 17/400\n",
      "88/88 [==============================] - 0s 937us/step - loss: 0.3363 - acc: 0.8889 - val_loss: 0.4286 - val_acc: 0.8889\n",
      "Epoch 18/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3313 - acc: 0.8889 - val_loss: 0.4277 - val_acc: 0.8889\n",
      "Epoch 19/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3362 - acc: 0.8889 - val_loss: 0.4255 - val_acc: 0.8889\n",
      "Epoch 20/400\n",
      "88/88 [==============================] - 0s 886us/step - loss: 0.3330 - acc: 0.8889 - val_loss: 0.4331 - val_acc: 0.8889\n",
      "Epoch 21/400\n",
      "88/88 [==============================] - 0s 991us/step - loss: 0.3270 - acc: 0.8889 - val_loss: 0.4413 - val_acc: 0.8889\n",
      "Epoch 22/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3281 - acc: 0.8889 - val_loss: 0.4416 - val_acc: 0.8889\n",
      "Epoch 23/400\n",
      "88/88 [==============================] - 0s 902us/step - loss: 0.3279 - acc: 0.8889 - val_loss: 0.4346 - val_acc: 0.8889\n",
      "Epoch 24/400\n",
      "88/88 [==============================] - 0s 882us/step - loss: 0.3272 - acc: 0.8889 - val_loss: 0.4368 - val_acc: 0.8889\n",
      "Epoch 25/400\n",
      "88/88 [==============================] - 0s 978us/step - loss: 0.3262 - acc: 0.8889 - val_loss: 0.4464 - val_acc: 0.8889\n",
      "Epoch 26/400\n",
      "88/88 [==============================] - 0s 957us/step - loss: 0.3234 - acc: 0.8889 - val_loss: 0.4493 - val_acc: 0.8889\n",
      "Epoch 27/400\n",
      "88/88 [==============================] - 0s 880us/step - loss: 0.3228 - acc: 0.8889 - val_loss: 0.4405 - val_acc: 0.8889\n",
      "Epoch 28/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3257 - acc: 0.8889 - val_loss: 0.4246 - val_acc: 0.8889\n",
      "Epoch 29/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3193 - acc: 0.8889 - val_loss: 0.4437 - val_acc: 0.8889\n",
      "Epoch 30/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3224 - acc: 0.8889 - val_loss: 0.4599 - val_acc: 0.8889\n",
      "Epoch 31/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3235 - acc: 0.8889 - val_loss: 0.4636 - val_acc: 0.8889\n",
      "Epoch 32/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3250 - acc: 0.8889 - val_loss: 0.4567 - val_acc: 0.8889\n",
      "Epoch 33/400\n",
      "88/88 [==============================] - 0s 899us/step - loss: 0.3183 - acc: 0.8889 - val_loss: 0.4409 - val_acc: 0.8889\n",
      "Epoch 34/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3191 - acc: 0.8889 - val_loss: 0.4545 - val_acc: 0.8889\n",
      "Epoch 35/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3211 - acc: 0.8889 - val_loss: 0.4690 - val_acc: 0.8889\n",
      "Epoch 36/400\n",
      "88/88 [==============================] - 0s 976us/step - loss: 0.3228 - acc: 0.8889 - val_loss: 0.4721 - val_acc: 0.8889\n",
      "Epoch 37/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3227 - acc: 0.8889 - val_loss: 0.4592 - val_acc: 0.8889\n",
      "Epoch 38/400\n",
      "88/88 [==============================] - 0s 969us/step - loss: 0.3154 - acc: 0.8889 - val_loss: 0.4554 - val_acc: 0.8889\n",
      "Epoch 39/400\n",
      "88/88 [==============================] - 0s 948us/step - loss: 0.3123 - acc: 0.8889 - val_loss: 0.4644 - val_acc: 0.8889\n",
      "Epoch 40/400\n",
      "88/88 [==============================] - 0s 960us/step - loss: 0.3165 - acc: 0.8889 - val_loss: 0.4743 - val_acc: 0.8889\n",
      "Epoch 41/400\n",
      "88/88 [==============================] - 0s 949us/step - loss: 0.3184 - acc: 0.8889 - val_loss: 0.4794 - val_acc: 0.8889\n",
      "Epoch 42/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3165 - acc: 0.8889 - val_loss: 0.4767 - val_acc: 0.8889\n",
      "Epoch 43/400\n",
      "88/88 [==============================] - 0s 950us/step - loss: 0.3065 - acc: 0.8889 - val_loss: 0.4566 - val_acc: 0.8889\n",
      "Epoch 44/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3097 - acc: 0.8889 - val_loss: 0.4665 - val_acc: 0.8889\n",
      "Epoch 45/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3150 - acc: 0.8889 - val_loss: 0.4862 - val_acc: 0.8889\n",
      "Epoch 46/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3138 - acc: 0.8889 - val_loss: 0.4869 - val_acc: 0.8889\n",
      "Epoch 47/400\n",
      "88/88 [==============================] - 0s 992us/step - loss: 0.3136 - acc: 0.8889 - val_loss: 0.4827 - val_acc: 0.8889\n",
      "Epoch 48/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3124 - acc: 0.8889 - val_loss: 0.4753 - val_acc: 0.8889\n",
      "Epoch 49/400\n",
      "88/88 [==============================] - 0s 981us/step - loss: 0.3120 - acc: 0.8889 - val_loss: 0.4809 - val_acc: 0.8889\n",
      "Epoch 50/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3129 - acc: 0.8889 - val_loss: 0.4701 - val_acc: 0.8889\n",
      "Epoch 51/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3033 - acc: 0.8889 - val_loss: 0.4488 - val_acc: 0.8889\n",
      "Epoch 52/400\n",
      "88/88 [==============================] - 0s 799us/step - loss: 0.3081 - acc: 0.8889 - val_loss: 0.4706 - val_acc: 0.8889\n",
      "Epoch 53/400\n",
      "88/88 [==============================] - 0s 834us/step - loss: 0.2993 - acc: 0.8889 - val_loss: 0.4587 - val_acc: 0.8889\n",
      "Epoch 54/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3037 - acc: 0.8889 - val_loss: 0.4819 - val_acc: 0.8889\n",
      "Epoch 55/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3064 - acc: 0.8864 - val_loss: 0.4713 - val_acc: 0.8889\n",
      "Epoch 56/400\n",
      "88/88 [==============================] - 0s 850us/step - loss: 0.2997 - acc: 0.8889 - val_loss: 0.4906 - val_acc: 0.8889\n",
      "Epoch 57/400\n",
      "88/88 [==============================] - 0s 941us/step - loss: 0.3043 - acc: 0.8889 - val_loss: 0.4994 - val_acc: 0.8889\n",
      "Epoch 58/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3025 - acc: 0.8902 - val_loss: 0.4807 - val_acc: 0.8889\n",
      "Epoch 59/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2926 - acc: 0.8889 - val_loss: 0.4704 - val_acc: 0.8889\n",
      "Epoch 60/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3005 - acc: 0.8889 - val_loss: 0.4815 - val_acc: 0.8889\n",
      "Epoch 61/400\n",
      "88/88 [==============================] - 0s 948us/step - loss: 0.2949 - acc: 0.8889 - val_loss: 0.4830 - val_acc: 0.8889\n",
      "Epoch 62/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2975 - acc: 0.8902 - val_loss: 0.4832 - val_acc: 0.8889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2905 - acc: 0.8902 - val_loss: 0.4809 - val_acc: 0.8889\n",
      "Epoch 64/400\n",
      "88/88 [==============================] - 0s 896us/step - loss: 0.3001 - acc: 0.8939 - val_loss: 0.5247 - val_acc: 0.8889\n",
      "Epoch 65/400\n",
      "88/88 [==============================] - 0s 802us/step - loss: 0.3023 - acc: 0.8927 - val_loss: 0.5294 - val_acc: 0.8889\n",
      "Epoch 66/400\n",
      "88/88 [==============================] - 0s 782us/step - loss: 0.2992 - acc: 0.8914 - val_loss: 0.4992 - val_acc: 0.8889\n",
      "Epoch 67/400\n",
      "88/88 [==============================] - 0s 765us/step - loss: 0.2919 - acc: 0.8902 - val_loss: 0.4384 - val_acc: 0.8889\n",
      "Epoch 68/400\n",
      "88/88 [==============================] - 0s 790us/step - loss: 0.3165 - acc: 0.8902 - val_loss: 0.4393 - val_acc: 0.8889\n",
      "Epoch 69/400\n",
      "88/88 [==============================] - 0s 869us/step - loss: 0.3015 - acc: 0.8914 - val_loss: 0.4999 - val_acc: 0.8889\n",
      "Epoch 70/400\n",
      "88/88 [==============================] - 0s 913us/step - loss: 0.2993 - acc: 0.8876 - val_loss: 0.5228 - val_acc: 0.8889\n",
      "Epoch 71/400\n",
      "88/88 [==============================] - 0s 984us/step - loss: 0.3082 - acc: 0.8889 - val_loss: 0.5231 - val_acc: 0.8889\n",
      "Epoch 72/400\n",
      "88/88 [==============================] - 0s 912us/step - loss: 0.3056 - acc: 0.8914 - val_loss: 0.5080 - val_acc: 0.8841\n",
      "Epoch 73/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2935 - acc: 0.8927 - val_loss: 0.4927 - val_acc: 0.8841\n",
      "Epoch 74/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3040 - acc: 0.8876 - val_loss: 0.4842 - val_acc: 0.8841\n",
      "Epoch 75/400\n",
      "88/88 [==============================] - 0s 937us/step - loss: 0.2874 - acc: 0.8977 - val_loss: 0.4696 - val_acc: 0.8889\n",
      "Epoch 76/400\n",
      "88/88 [==============================] - 0s 773us/step - loss: 0.2964 - acc: 0.8914 - val_loss: 0.4676 - val_acc: 0.8889\n",
      "Epoch 77/400\n",
      "88/88 [==============================] - 0s 839us/step - loss: 0.2952 - acc: 0.8864 - val_loss: 0.4781 - val_acc: 0.8889\n",
      "Epoch 78/400\n",
      "88/88 [==============================] - 0s 771us/step - loss: 0.2946 - acc: 0.8952 - val_loss: 0.4874 - val_acc: 0.8841\n",
      "Epoch 79/400\n",
      "88/88 [==============================] - 0s 839us/step - loss: 0.2821 - acc: 0.8952 - val_loss: 0.4922 - val_acc: 0.8841\n",
      "Epoch 80/400\n",
      "88/88 [==============================] - 0s 798us/step - loss: 0.2770 - acc: 0.8952 - val_loss: 0.4919 - val_acc: 0.8841\n",
      "Epoch 81/400\n",
      "88/88 [==============================] - 0s 933us/step - loss: 0.2806 - acc: 0.8927 - val_loss: 0.4904 - val_acc: 0.8889\n",
      "Epoch 82/400\n",
      "88/88 [==============================] - 0s 925us/step - loss: 0.2849 - acc: 0.8914 - val_loss: 0.4876 - val_acc: 0.8841\n",
      "Epoch 83/400\n",
      "88/88 [==============================] - 0s 953us/step - loss: 0.2801 - acc: 0.8939 - val_loss: 0.4851 - val_acc: 0.8841\n",
      "Epoch 84/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2724 - acc: 0.9003 - val_loss: 0.4961 - val_acc: 0.8744\n",
      "Epoch 85/400\n",
      "88/88 [==============================] - 0s 989us/step - loss: 0.2852 - acc: 0.8952 - val_loss: 0.5071 - val_acc: 0.8792\n",
      "Epoch 86/400\n",
      "88/88 [==============================] - 0s 979us/step - loss: 0.2774 - acc: 0.8965 - val_loss: 0.4854 - val_acc: 0.8841\n",
      "Epoch 87/400\n",
      "88/88 [==============================] - 0s 898us/step - loss: 0.2850 - acc: 0.8990 - val_loss: 0.5000 - val_acc: 0.8841\n",
      "Epoch 88/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2773 - acc: 0.8965 - val_loss: 0.4853 - val_acc: 0.8841\n",
      "Epoch 89/400\n",
      "88/88 [==============================] - 0s 630us/step - loss: 0.2680 - acc: 0.9003 - val_loss: 0.4799 - val_acc: 0.8841\n",
      "Epoch 90/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2683 - acc: 0.8965 - val_loss: 0.5012 - val_acc: 0.8841\n",
      "Epoch 91/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2729 - acc: 0.8965 - val_loss: 0.5061 - val_acc: 0.8841\n",
      "Epoch 92/400\n",
      "88/88 [==============================] - 0s 769us/step - loss: 0.2701 - acc: 0.8977 - val_loss: 0.4958 - val_acc: 0.8841\n",
      "Epoch 93/400\n",
      "88/88 [==============================] - 0s 769us/step - loss: 0.2662 - acc: 0.8990 - val_loss: 0.4669 - val_acc: 0.8841\n",
      "Epoch 94/400\n",
      "88/88 [==============================] - 0s 831us/step - loss: 0.2752 - acc: 0.8977 - val_loss: 0.5184 - val_acc: 0.8792\n",
      "Epoch 95/400\n",
      "88/88 [==============================] - 0s 856us/step - loss: 0.2597 - acc: 0.9015 - val_loss: 0.5113 - val_acc: 0.8841\n",
      "Epoch 96/400\n",
      "88/88 [==============================] - 0s 905us/step - loss: 0.2604 - acc: 0.8990 - val_loss: 0.4719 - val_acc: 0.8841\n",
      "Epoch 97/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2598 - acc: 0.8952 - val_loss: 0.5086 - val_acc: 0.8744\n",
      "Epoch 98/400\n",
      "88/88 [==============================] - 0s 903us/step - loss: 0.2585 - acc: 0.8990 - val_loss: 0.5136 - val_acc: 0.8744\n",
      "Epoch 99/400\n",
      "88/88 [==============================] - 0s 878us/step - loss: 0.2544 - acc: 0.9015 - val_loss: 0.4777 - val_acc: 0.8841\n",
      "Epoch 100/400\n",
      "88/88 [==============================] - 0s 943us/step - loss: 0.2623 - acc: 0.8977 - val_loss: 0.5151 - val_acc: 0.8792\n",
      "Epoch 101/400\n",
      "88/88 [==============================] - 0s 862us/step - loss: 0.2589 - acc: 0.9015 - val_loss: 0.5086 - val_acc: 0.8792\n",
      "Epoch 102/400\n",
      "88/88 [==============================] - 0s 999us/step - loss: 0.2571 - acc: 0.8965 - val_loss: 0.4792 - val_acc: 0.8792\n",
      "Epoch 103/400\n",
      "88/88 [==============================] - 0s 937us/step - loss: 0.2503 - acc: 0.9015 - val_loss: 0.4942 - val_acc: 0.8792\n",
      "Epoch 104/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2598 - acc: 0.9003 - val_loss: 0.4898 - val_acc: 0.8792\n",
      "Epoch 105/400\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2471 - acc: 0.9078 - val_loss: 0.4966 - val_acc: 0.8792\n",
      "Epoch 106/400\n",
      "88/88 [==============================] - 0s 991us/step - loss: 0.2542 - acc: 0.9003 - val_loss: 0.5230 - val_acc: 0.8744\n",
      "Epoch 107/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2473 - acc: 0.9015 - val_loss: 0.4966 - val_acc: 0.8744\n",
      "Epoch 108/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2462 - acc: 0.9003 - val_loss: 0.4884 - val_acc: 0.8744\n",
      "Epoch 109/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2539 - acc: 0.8965 - val_loss: 0.5296 - val_acc: 0.8792\n",
      "Epoch 110/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2440 - acc: 0.9053 - val_loss: 0.5146 - val_acc: 0.8647\n",
      "Epoch 111/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2409 - acc: 0.9129 - val_loss: 0.4965 - val_acc: 0.8696\n",
      "Epoch 112/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2464 - acc: 0.9003 - val_loss: 0.5291 - val_acc: 0.8696\n",
      "Epoch 113/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2608 - acc: 0.9003 - val_loss: 0.5266 - val_acc: 0.8696\n",
      "Epoch 114/400\n",
      "88/88 [==============================] - 0s 994us/step - loss: 0.2475 - acc: 0.9015 - val_loss: 0.5001 - val_acc: 0.8792\n",
      "Epoch 115/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2412 - acc: 0.9040 - val_loss: 0.4906 - val_acc: 0.8792\n",
      "Epoch 116/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2450 - acc: 0.9040 - val_loss: 0.5517 - val_acc: 0.8744\n",
      "Epoch 117/400\n",
      "88/88 [==============================] - 0s 806us/step - loss: 0.2551 - acc: 0.8977 - val_loss: 0.5346 - val_acc: 0.8647\n",
      "Epoch 118/400\n",
      "88/88 [==============================] - 0s 880us/step - loss: 0.2469 - acc: 0.9015 - val_loss: 0.4698 - val_acc: 0.8744\n",
      "Epoch 119/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2638 - acc: 0.8965 - val_loss: 0.5127 - val_acc: 0.8647\n",
      "Epoch 120/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2515 - acc: 0.8914 - val_loss: 0.5465 - val_acc: 0.8792\n",
      "Epoch 121/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2584 - acc: 0.8965 - val_loss: 0.5418 - val_acc: 0.8696\n",
      "Epoch 122/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2678 - acc: 0.8990 - val_loss: 0.5203 - val_acc: 0.8744\n",
      "Epoch 123/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2304 - acc: 0.9078 - val_loss: 0.4831 - val_acc: 0.8744\n",
      "Epoch 124/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.3120 - acc: 0.8838 - val_loss: 0.4860 - val_acc: 0.8841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/400\n",
      "88/88 [==============================] - 0s 914us/step - loss: 0.2643 - acc: 0.9015 - val_loss: 0.5354 - val_acc: 0.8599\n",
      "Epoch 126/400\n",
      "88/88 [==============================] - 0s 887us/step - loss: 0.2604 - acc: 0.9040 - val_loss: 0.5595 - val_acc: 0.8792\n",
      "Epoch 127/400\n",
      "88/88 [==============================] - 0s 797us/step - loss: 0.2765 - acc: 0.8965 - val_loss: 0.5539 - val_acc: 0.8841\n",
      "Epoch 128/400\n",
      "88/88 [==============================] - 0s 792us/step - loss: 0.2746 - acc: 0.8939 - val_loss: 0.5305 - val_acc: 0.8841\n",
      "Epoch 129/400\n",
      "88/88 [==============================] - 0s 956us/step - loss: 0.2188 - acc: 0.9091 - val_loss: 0.4792 - val_acc: 0.8696\n",
      "Epoch 130/400\n",
      "88/88 [==============================] - 0s 898us/step - loss: 0.2769 - acc: 0.8889 - val_loss: 0.5016 - val_acc: 0.8744\n",
      "Epoch 131/400\n",
      "88/88 [==============================] - 0s 924us/step - loss: 0.2231 - acc: 0.9129 - val_loss: 0.5408 - val_acc: 0.8841\n",
      "Epoch 132/400\n",
      "88/88 [==============================] - 0s 810us/step - loss: 0.2387 - acc: 0.9078 - val_loss: 0.5476 - val_acc: 0.8792\n",
      "Epoch 133/400\n",
      "88/88 [==============================] - 0s 797us/step - loss: 0.2543 - acc: 0.9015 - val_loss: 0.5388 - val_acc: 0.8792\n",
      "Epoch 134/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2296 - acc: 0.9066 - val_loss: 0.5177 - val_acc: 0.8792\n",
      "Epoch 135/400\n",
      "88/88 [==============================] - 0s 824us/step - loss: 0.2263 - acc: 0.9129 - val_loss: 0.4928 - val_acc: 0.8792\n",
      "Epoch 136/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2538 - acc: 0.8952 - val_loss: 0.5018 - val_acc: 0.8792\n",
      "Epoch 137/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2129 - acc: 0.9129 - val_loss: 0.5234 - val_acc: 0.8744\n",
      "Epoch 138/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2170 - acc: 0.9179 - val_loss: 0.5333 - val_acc: 0.8744\n",
      "Epoch 139/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2333 - acc: 0.9066 - val_loss: 0.5304 - val_acc: 0.8599\n",
      "Epoch 140/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2317 - acc: 0.9053 - val_loss: 0.5111 - val_acc: 0.8647\n",
      "Epoch 141/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2331 - acc: 0.9091 - val_loss: 0.5001 - val_acc: 0.8647\n",
      "Epoch 142/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2150 - acc: 0.9141 - val_loss: 0.5083 - val_acc: 0.8744\n",
      "Epoch 143/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2241 - acc: 0.9066 - val_loss: 0.4944 - val_acc: 0.8744\n",
      "Epoch 144/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2294 - acc: 0.9154 - val_loss: 0.5308 - val_acc: 0.8647\n",
      "Epoch 145/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2213 - acc: 0.9116 - val_loss: 0.5623 - val_acc: 0.8599\n",
      "Epoch 146/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2377 - acc: 0.8965 - val_loss: 0.5366 - val_acc: 0.8696\n",
      "Epoch 147/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2134 - acc: 0.9179 - val_loss: 0.5139 - val_acc: 0.8792\n",
      "Epoch 148/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2107 - acc: 0.9167 - val_loss: 0.5180 - val_acc: 0.8744\n",
      "Epoch 149/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2198 - acc: 0.9116 - val_loss: 0.5547 - val_acc: 0.8696\n",
      "Epoch 150/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2152 - acc: 0.9104 - val_loss: 0.5258 - val_acc: 0.8599\n",
      "Epoch 151/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2361 - acc: 0.9104 - val_loss: 0.5219 - val_acc: 0.8551\n",
      "Epoch 152/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2325 - acc: 0.9078 - val_loss: 0.5398 - val_acc: 0.8647\n",
      "Epoch 153/400\n",
      "88/88 [==============================] - 0s 997us/step - loss: 0.2250 - acc: 0.9078 - val_loss: 0.5276 - val_acc: 0.8599\n",
      "Epoch 154/400\n",
      "88/88 [==============================] - 0s 938us/step - loss: 0.2198 - acc: 0.9179 - val_loss: 0.5013 - val_acc: 0.8647\n",
      "Epoch 155/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2220 - acc: 0.9116 - val_loss: 0.5428 - val_acc: 0.8599\n",
      "Epoch 156/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2254 - acc: 0.9078 - val_loss: 0.5649 - val_acc: 0.8647\n",
      "Epoch 157/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2260 - acc: 0.9040 - val_loss: 0.5469 - val_acc: 0.8696\n",
      "Epoch 158/400\n",
      "88/88 [==============================] - 0s 963us/step - loss: 0.1964 - acc: 0.9167 - val_loss: 0.5103 - val_acc: 0.8696\n",
      "Epoch 159/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2389 - acc: 0.9015 - val_loss: 0.5392 - val_acc: 0.8599\n",
      "Epoch 160/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2183 - acc: 0.9154 - val_loss: 0.5694 - val_acc: 0.8696\n",
      "Epoch 161/400\n",
      "88/88 [==============================] - 0s 970us/step - loss: 0.2215 - acc: 0.9116 - val_loss: 0.5682 - val_acc: 0.8647\n",
      "Epoch 162/400\n",
      "88/88 [==============================] - 0s 992us/step - loss: 0.2107 - acc: 0.9104 - val_loss: 0.5547 - val_acc: 0.8599\n",
      "Epoch 163/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2124 - acc: 0.9192 - val_loss: 0.5409 - val_acc: 0.8647\n",
      "Epoch 164/400\n",
      "88/88 [==============================] - 0s 976us/step - loss: 0.2081 - acc: 0.9230 - val_loss: 0.5364 - val_acc: 0.8599\n",
      "Epoch 165/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2200 - acc: 0.9205 - val_loss: 0.5304 - val_acc: 0.8647\n",
      "Epoch 166/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2120 - acc: 0.9116 - val_loss: 0.5462 - val_acc: 0.8551\n",
      "Epoch 167/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2026 - acc: 0.9192 - val_loss: 0.5600 - val_acc: 0.8599\n",
      "Epoch 168/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2139 - acc: 0.9154 - val_loss: 0.5546 - val_acc: 0.8599\n",
      "Epoch 169/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1931 - acc: 0.9280 - val_loss: 0.5420 - val_acc: 0.8647\n",
      "Epoch 170/400\n",
      "88/88 [==============================] - 0s 878us/step - loss: 0.2153 - acc: 0.9104 - val_loss: 0.5401 - val_acc: 0.8647\n",
      "Epoch 171/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1981 - acc: 0.9205 - val_loss: 0.5603 - val_acc: 0.8599\n",
      "Epoch 172/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2108 - acc: 0.9179 - val_loss: 0.5756 - val_acc: 0.8599\n",
      "Epoch 173/400\n",
      "88/88 [==============================] - 0s 957us/step - loss: 0.2044 - acc: 0.9091 - val_loss: 0.5632 - val_acc: 0.8599\n",
      "Epoch 174/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2019 - acc: 0.9129 - val_loss: 0.5428 - val_acc: 0.8502\n",
      "Epoch 175/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2040 - acc: 0.9230 - val_loss: 0.5363 - val_acc: 0.8502\n",
      "Epoch 176/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1895 - acc: 0.9230 - val_loss: 0.5400 - val_acc: 0.8502\n",
      "Epoch 177/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1977 - acc: 0.9217 - val_loss: 0.5415 - val_acc: 0.8551\n",
      "Epoch 178/400\n",
      "88/88 [==============================] - 0s 966us/step - loss: 0.1916 - acc: 0.9255 - val_loss: 0.5356 - val_acc: 0.8599\n",
      "Epoch 179/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2027 - acc: 0.9192 - val_loss: 0.5418 - val_acc: 0.8599\n",
      "Epoch 180/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1898 - acc: 0.9268 - val_loss: 0.5195 - val_acc: 0.8647\n",
      "Epoch 181/400\n",
      "88/88 [==============================] - 0s 920us/step - loss: 0.2001 - acc: 0.9192 - val_loss: 0.5507 - val_acc: 0.8744\n",
      "Epoch 182/400\n",
      "88/88 [==============================] - 0s 879us/step - loss: 0.1879 - acc: 0.9230 - val_loss: 0.5601 - val_acc: 0.8647\n",
      "Epoch 183/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1893 - acc: 0.9205 - val_loss: 0.5571 - val_acc: 0.8599\n",
      "Epoch 184/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1930 - acc: 0.9268 - val_loss: 0.5406 - val_acc: 0.8502\n",
      "Epoch 185/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1902 - acc: 0.9255 - val_loss: 0.5577 - val_acc: 0.8599\n",
      "Epoch 186/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1863 - acc: 0.9192 - val_loss: 0.5557 - val_acc: 0.8502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 187/400\n",
      "88/88 [==============================] - 0s 819us/step - loss: 0.1872 - acc: 0.9230 - val_loss: 0.5599 - val_acc: 0.8502\n",
      "Epoch 188/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1925 - acc: 0.9242 - val_loss: 0.5976 - val_acc: 0.8502\n",
      "Epoch 189/400\n",
      "88/88 [==============================] - 0s 990us/step - loss: 0.2221 - acc: 0.9053 - val_loss: 0.5544 - val_acc: 0.8599\n",
      "Epoch 190/400\n",
      "88/88 [==============================] - 0s 935us/step - loss: 0.1817 - acc: 0.9268 - val_loss: 0.5120 - val_acc: 0.8647\n",
      "Epoch 191/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2226 - acc: 0.9066 - val_loss: 0.5441 - val_acc: 0.8696\n",
      "Epoch 192/400\n",
      "88/88 [==============================] - 0s 985us/step - loss: 0.1980 - acc: 0.9217 - val_loss: 0.6026 - val_acc: 0.8454\n",
      "Epoch 193/400\n",
      "88/88 [==============================] - 0s 840us/step - loss: 0.2216 - acc: 0.9129 - val_loss: 0.5835 - val_acc: 0.8599\n",
      "Epoch 194/400\n",
      "88/88 [==============================] - 0s 846us/step - loss: 0.1886 - acc: 0.9167 - val_loss: 0.5364 - val_acc: 0.8502\n",
      "Epoch 195/400\n",
      "88/88 [==============================] - 0s 768us/step - loss: 0.2135 - acc: 0.9091 - val_loss: 0.5584 - val_acc: 0.8599\n",
      "Epoch 196/400\n",
      "88/88 [==============================] - 0s 777us/step - loss: 0.1924 - acc: 0.9268 - val_loss: 0.6288 - val_acc: 0.8454\n",
      "Epoch 197/400\n",
      "88/88 [==============================] - 0s 837us/step - loss: 0.2287 - acc: 0.9179 - val_loss: 0.5797 - val_acc: 0.8696\n",
      "Epoch 198/400\n",
      "88/88 [==============================] - 0s 839us/step - loss: 0.1801 - acc: 0.9331 - val_loss: 0.5164 - val_acc: 0.8647\n",
      "Epoch 199/400\n",
      "88/88 [==============================] - 0s 761us/step - loss: 0.2383 - acc: 0.9091 - val_loss: 0.5085 - val_acc: 0.8647\n",
      "Epoch 200/400\n",
      "88/88 [==============================] - 0s 714us/step - loss: 0.2396 - acc: 0.9028 - val_loss: 0.5403 - val_acc: 0.8647\n",
      "Epoch 201/400\n",
      "88/88 [==============================] - 0s 896us/step - loss: 0.1786 - acc: 0.9280 - val_loss: 0.5862 - val_acc: 0.8357\n",
      "Epoch 202/400\n",
      "88/88 [==============================] - 0s 968us/step - loss: 0.2500 - acc: 0.9078 - val_loss: 0.5986 - val_acc: 0.8406\n",
      "Epoch 203/400\n",
      "88/88 [==============================] - 0s 909us/step - loss: 0.2542 - acc: 0.9040 - val_loss: 0.5709 - val_acc: 0.8647\n",
      "Epoch 204/400\n",
      "88/88 [==============================] - 0s 866us/step - loss: 0.1913 - acc: 0.9154 - val_loss: 0.5207 - val_acc: 0.8599\n",
      "Epoch 205/400\n",
      "88/88 [==============================] - 0s 861us/step - loss: 0.2345 - acc: 0.8990 - val_loss: 0.5195 - val_acc: 0.8599\n",
      "Epoch 206/400\n",
      "88/88 [==============================] - 0s 815us/step - loss: 0.2514 - acc: 0.8939 - val_loss: 0.5586 - val_acc: 0.8502\n",
      "Epoch 207/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1982 - acc: 0.9230 - val_loss: 0.6284 - val_acc: 0.8599\n",
      "Epoch 208/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2127 - acc: 0.9167 - val_loss: 0.6501 - val_acc: 0.8454\n",
      "Epoch 209/400\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2297 - acc: 0.9154 - val_loss: 0.5865 - val_acc: 0.8599\n",
      "Epoch 210/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1756 - acc: 0.9306 - val_loss: 0.5371 - val_acc: 0.8599\n",
      "Epoch 211/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1980 - acc: 0.9192 - val_loss: 0.5412 - val_acc: 0.8647\n",
      "Epoch 212/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1871 - acc: 0.9230 - val_loss: 0.5720 - val_acc: 0.8599\n",
      "Epoch 213/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1909 - acc: 0.9230 - val_loss: 0.5988 - val_acc: 0.8744\n",
      "Epoch 214/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1934 - acc: 0.9205 - val_loss: 0.5928 - val_acc: 0.8744\n",
      "Epoch 215/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1795 - acc: 0.9293 - val_loss: 0.5600 - val_acc: 0.8502\n",
      "Epoch 216/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1917 - acc: 0.9167 - val_loss: 0.5563 - val_acc: 0.8502\n",
      "Epoch 217/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1727 - acc: 0.9331 - val_loss: 0.5987 - val_acc: 0.8599\n",
      "Epoch 218/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1641 - acc: 0.9356 - val_loss: 0.6158 - val_acc: 0.8599\n",
      "Epoch 219/400\n",
      "88/88 [==============================] - 0s 804us/step - loss: 0.1949 - acc: 0.9179 - val_loss: 0.5850 - val_acc: 0.8599\n",
      "Epoch 220/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1780 - acc: 0.9255 - val_loss: 0.5370 - val_acc: 0.8647\n",
      "Epoch 221/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1962 - acc: 0.9242 - val_loss: 0.5524 - val_acc: 0.8696\n",
      "Epoch 222/400\n",
      "88/88 [==============================] - 0s 724us/step - loss: 0.1858 - acc: 0.9230 - val_loss: 0.5992 - val_acc: 0.8647\n",
      "Epoch 223/400\n",
      "88/88 [==============================] - 0s 985us/step - loss: 0.1985 - acc: 0.9205 - val_loss: 0.6204 - val_acc: 0.8599\n",
      "Epoch 224/400\n",
      "88/88 [==============================] - 0s 994us/step - loss: 0.2075 - acc: 0.9179 - val_loss: 0.5966 - val_acc: 0.8599\n",
      "Epoch 225/400\n",
      "88/88 [==============================] - 0s 990us/step - loss: 0.1815 - acc: 0.9242 - val_loss: 0.5573 - val_acc: 0.8744\n",
      "Epoch 226/400\n",
      "88/88 [==============================] - 0s 991us/step - loss: 0.1834 - acc: 0.9230 - val_loss: 0.5577 - val_acc: 0.8502\n",
      "Epoch 227/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1711 - acc: 0.9331 - val_loss: 0.5759 - val_acc: 0.8599\n",
      "Epoch 228/400\n",
      "88/88 [==============================] - 0s 936us/step - loss: 0.1736 - acc: 0.9306 - val_loss: 0.5690 - val_acc: 0.8454\n",
      "Epoch 229/400\n",
      "88/88 [==============================] - 0s 859us/step - loss: 0.1717 - acc: 0.9280 - val_loss: 0.5763 - val_acc: 0.8454\n",
      "Epoch 230/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1704 - acc: 0.9293 - val_loss: 0.5943 - val_acc: 0.8502\n",
      "Epoch 231/400\n",
      "88/88 [==============================] - 0s 919us/step - loss: 0.1793 - acc: 0.9255 - val_loss: 0.5750 - val_acc: 0.8406\n",
      "Epoch 232/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1849 - acc: 0.9205 - val_loss: 0.5535 - val_acc: 0.8551\n",
      "Epoch 233/400\n",
      "88/88 [==============================] - 0s 934us/step - loss: 0.1783 - acc: 0.9343 - val_loss: 0.5889 - val_acc: 0.8551\n",
      "Epoch 234/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1696 - acc: 0.9318 - val_loss: 0.5958 - val_acc: 0.8647\n",
      "Epoch 235/400\n",
      "88/88 [==============================] - 0s 875us/step - loss: 0.1735 - acc: 0.9318 - val_loss: 0.5668 - val_acc: 0.8647\n",
      "Epoch 236/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1727 - acc: 0.9293 - val_loss: 0.5363 - val_acc: 0.8744\n",
      "Epoch 237/400\n",
      "88/88 [==============================] - 0s 887us/step - loss: 0.1815 - acc: 0.9306 - val_loss: 0.5672 - val_acc: 0.8551\n",
      "Epoch 238/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1699 - acc: 0.9331 - val_loss: 0.5907 - val_acc: 0.8647\n",
      "Epoch 239/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1831 - acc: 0.9255 - val_loss: 0.5745 - val_acc: 0.8599\n",
      "Epoch 240/400\n",
      "88/88 [==============================] - 0s 870us/step - loss: 0.1657 - acc: 0.9369 - val_loss: 0.5620 - val_acc: 0.8696\n",
      "Epoch 241/400\n",
      "88/88 [==============================] - 0s 834us/step - loss: 0.1713 - acc: 0.9356 - val_loss: 0.5840 - val_acc: 0.8551\n",
      "Epoch 242/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1614 - acc: 0.9331 - val_loss: 0.6003 - val_acc: 0.8551\n",
      "Epoch 243/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1636 - acc: 0.9293 - val_loss: 0.6145 - val_acc: 0.8551\n",
      "Epoch 244/400\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.1539 - acc: 0.9356 - val_loss: 0.6258 - val_acc: 0.8502\n",
      "Epoch 245/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1604 - acc: 0.9381 - val_loss: 0.6205 - val_acc: 0.8357\n",
      "Epoch 246/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1675 - acc: 0.9343 - val_loss: 0.6126 - val_acc: 0.8406\n",
      "Epoch 247/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1602 - acc: 0.9331 - val_loss: 0.6195 - val_acc: 0.8551\n",
      "Epoch 248/400\n",
      "88/88 [==============================] - 0s 887us/step - loss: 0.1586 - acc: 0.9356 - val_loss: 0.6063 - val_acc: 0.8551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 249/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1555 - acc: 0.9394 - val_loss: 0.5958 - val_acc: 0.8502\n",
      "Epoch 250/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1449 - acc: 0.9444 - val_loss: 0.5747 - val_acc: 0.8647\n",
      "Epoch 251/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1659 - acc: 0.9306 - val_loss: 0.6010 - val_acc: 0.8647\n",
      "Epoch 252/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1564 - acc: 0.9419 - val_loss: 0.6204 - val_acc: 0.8599\n",
      "Epoch 253/400\n",
      "88/88 [==============================] - 0s 943us/step - loss: 0.1539 - acc: 0.9369 - val_loss: 0.6017 - val_acc: 0.8406\n",
      "Epoch 254/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1549 - acc: 0.9381 - val_loss: 0.5754 - val_acc: 0.8357\n",
      "Epoch 255/400\n",
      "88/88 [==============================] - 0s 965us/step - loss: 0.1581 - acc: 0.9432 - val_loss: 0.6167 - val_acc: 0.8357\n",
      "Epoch 256/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1604 - acc: 0.9331 - val_loss: 0.6492 - val_acc: 0.8357\n",
      "Epoch 257/400\n",
      "88/88 [==============================] - 0s 999us/step - loss: 0.1673 - acc: 0.9331 - val_loss: 0.6378 - val_acc: 0.8406\n",
      "Epoch 258/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1602 - acc: 0.9343 - val_loss: 0.6239 - val_acc: 0.8357\n",
      "Epoch 259/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1497 - acc: 0.9419 - val_loss: 0.6258 - val_acc: 0.8551\n",
      "Epoch 260/400\n",
      "88/88 [==============================] - 0s 955us/step - loss: 0.1496 - acc: 0.9470 - val_loss: 0.6357 - val_acc: 0.8502\n",
      "Epoch 261/400\n",
      "88/88 [==============================] - 0s 997us/step - loss: 0.1654 - acc: 0.9306 - val_loss: 0.6026 - val_acc: 0.8599\n",
      "Epoch 262/400\n",
      "88/88 [==============================] - 0s 958us/step - loss: 0.1539 - acc: 0.9394 - val_loss: 0.5699 - val_acc: 0.8454\n",
      "Epoch 263/400\n",
      "88/88 [==============================] - 0s 947us/step - loss: 0.1716 - acc: 0.9318 - val_loss: 0.6011 - val_acc: 0.8454\n",
      "Epoch 264/400\n",
      "88/88 [==============================] - 0s 906us/step - loss: 0.1482 - acc: 0.9457 - val_loss: 0.6408 - val_acc: 0.8551\n",
      "Epoch 265/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1866 - acc: 0.9217 - val_loss: 0.6297 - val_acc: 0.8502\n",
      "Epoch 266/400\n",
      "88/88 [==============================] - 0s 992us/step - loss: 0.1463 - acc: 0.9394 - val_loss: 0.5851 - val_acc: 0.8309\n",
      "Epoch 267/400\n",
      "88/88 [==============================] - 0s 959us/step - loss: 0.2015 - acc: 0.9242 - val_loss: 0.6385 - val_acc: 0.8406\n",
      "Epoch 268/400\n",
      "88/88 [==============================] - 0s 971us/step - loss: 0.1548 - acc: 0.9444 - val_loss: 0.6435 - val_acc: 0.8551\n",
      "Epoch 269/400\n",
      "88/88 [==============================] - 0s 894us/step - loss: 0.1886 - acc: 0.9242 - val_loss: 0.6362 - val_acc: 0.8454\n",
      "Epoch 270/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1591 - acc: 0.9242 - val_loss: 0.5498 - val_acc: 0.8406\n",
      "Epoch 271/400\n",
      "88/88 [==============================] - 0s 969us/step - loss: 0.1730 - acc: 0.9381 - val_loss: 0.5652 - val_acc: 0.8406\n",
      "Epoch 272/400\n",
      "88/88 [==============================] - 0s 965us/step - loss: 0.1514 - acc: 0.9432 - val_loss: 0.6798 - val_acc: 0.8213\n",
      "Epoch 273/400\n",
      "88/88 [==============================] - 0s 907us/step - loss: 0.1887 - acc: 0.9154 - val_loss: 0.6492 - val_acc: 0.8502\n",
      "Epoch 274/400\n",
      "88/88 [==============================] - 0s 985us/step - loss: 0.1880 - acc: 0.9217 - val_loss: 0.6239 - val_acc: 0.8309\n",
      "Epoch 275/400\n",
      "88/88 [==============================] - 0s 917us/step - loss: 0.1734 - acc: 0.9293 - val_loss: 0.6004 - val_acc: 0.8261\n",
      "Epoch 276/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1679 - acc: 0.9369 - val_loss: 0.6345 - val_acc: 0.8406\n",
      "Epoch 277/400\n",
      "88/88 [==============================] - 0s 962us/step - loss: 0.1604 - acc: 0.9293 - val_loss: 0.6922 - val_acc: 0.8357\n",
      "Epoch 278/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1654 - acc: 0.9318 - val_loss: 0.6864 - val_acc: 0.8454\n",
      "Epoch 279/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1694 - acc: 0.9306 - val_loss: 0.6541 - val_acc: 0.8502\n",
      "Epoch 280/400\n",
      "88/88 [==============================] - 0s 908us/step - loss: 0.1656 - acc: 0.9369 - val_loss: 0.6142 - val_acc: 0.8357\n",
      "Epoch 281/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1430 - acc: 0.9432 - val_loss: 0.5824 - val_acc: 0.8309\n",
      "Epoch 282/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1545 - acc: 0.9394 - val_loss: 0.6147 - val_acc: 0.8454\n",
      "Epoch 283/400\n",
      "88/88 [==============================] - 0s 915us/step - loss: 0.1510 - acc: 0.9381 - val_loss: 0.6257 - val_acc: 0.8454\n",
      "Epoch 284/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1568 - acc: 0.9432 - val_loss: 0.6051 - val_acc: 0.8406\n",
      "Epoch 285/400\n",
      "88/88 [==============================] - 0s 934us/step - loss: 0.1556 - acc: 0.9457 - val_loss: 0.6002 - val_acc: 0.8309\n",
      "Epoch 286/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1452 - acc: 0.9432 - val_loss: 0.6399 - val_acc: 0.8309\n",
      "Epoch 287/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1887 - acc: 0.9192 - val_loss: 0.6259 - val_acc: 0.8357\n",
      "Epoch 288/400\n",
      "88/88 [==============================] - 0s 865us/step - loss: 0.1540 - acc: 0.9343 - val_loss: 0.5952 - val_acc: 0.8357\n",
      "Epoch 289/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1629 - acc: 0.9369 - val_loss: 0.5935 - val_acc: 0.8357\n",
      "Epoch 290/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1359 - acc: 0.9520 - val_loss: 0.6048 - val_acc: 0.8357\n",
      "Epoch 291/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1433 - acc: 0.9432 - val_loss: 0.6138 - val_acc: 0.8454\n",
      "Epoch 292/400\n",
      "88/88 [==============================] - 0s 894us/step - loss: 0.1292 - acc: 0.9520 - val_loss: 0.6149 - val_acc: 0.8406\n",
      "Epoch 293/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1548 - acc: 0.9356 - val_loss: 0.6277 - val_acc: 0.8502\n",
      "Epoch 294/400\n",
      "88/88 [==============================] - 0s 932us/step - loss: 0.1347 - acc: 0.9495 - val_loss: 0.6348 - val_acc: 0.8551\n",
      "Epoch 295/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1473 - acc: 0.9318 - val_loss: 0.6019 - val_acc: 0.8309\n",
      "Epoch 296/400\n",
      "88/88 [==============================] - 0s 984us/step - loss: 0.1394 - acc: 0.9495 - val_loss: 0.6173 - val_acc: 0.8406\n",
      "Epoch 297/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1642 - acc: 0.9369 - val_loss: 0.6374 - val_acc: 0.8599\n",
      "Epoch 298/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1394 - acc: 0.9407 - val_loss: 0.6545 - val_acc: 0.8406\n",
      "Epoch 299/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1555 - acc: 0.9356 - val_loss: 0.6119 - val_acc: 0.8454\n",
      "Epoch 300/400\n",
      "88/88 [==============================] - 0s 943us/step - loss: 0.1345 - acc: 0.9482 - val_loss: 0.6047 - val_acc: 0.8502\n",
      "Epoch 301/400\n",
      "88/88 [==============================] - 0s 856us/step - loss: 0.1470 - acc: 0.9470 - val_loss: 0.6168 - val_acc: 0.8454\n",
      "Epoch 302/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1471 - acc: 0.9444 - val_loss: 0.5993 - val_acc: 0.8454\n",
      "Epoch 303/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1329 - acc: 0.9470 - val_loss: 0.6125 - val_acc: 0.8309\n",
      "Epoch 304/400\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.1435 - acc: 0.9432 - val_loss: 0.6120 - val_acc: 0.8213\n",
      "Epoch 305/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1234 - acc: 0.9533 - val_loss: 0.6560 - val_acc: 0.8261\n",
      "Epoch 306/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1430 - acc: 0.9457 - val_loss: 0.6717 - val_acc: 0.8357\n",
      "Epoch 307/400\n",
      "88/88 [==============================] - 0s 926us/step - loss: 0.1334 - acc: 0.9508 - val_loss: 0.6571 - val_acc: 0.8261\n",
      "Epoch 308/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1435 - acc: 0.9369 - val_loss: 0.6567 - val_acc: 0.8261\n",
      "Epoch 309/400\n",
      "88/88 [==============================] - 0s 916us/step - loss: 0.1314 - acc: 0.9457 - val_loss: 0.6678 - val_acc: 0.8502\n",
      "Epoch 310/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1342 - acc: 0.9495 - val_loss: 0.6437 - val_acc: 0.8502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 311/400\n",
      "88/88 [==============================] - 0s 962us/step - loss: 0.1295 - acc: 0.9495 - val_loss: 0.5925 - val_acc: 0.8551\n",
      "Epoch 312/400\n",
      "88/88 [==============================] - 0s 794us/step - loss: 0.1401 - acc: 0.9444 - val_loss: 0.6107 - val_acc: 0.8551\n",
      "Epoch 313/400\n",
      "88/88 [==============================] - 0s 801us/step - loss: 0.1535 - acc: 0.9407 - val_loss: 0.6611 - val_acc: 0.8502\n",
      "Epoch 314/400\n",
      "88/88 [==============================] - 0s 772us/step - loss: 0.1325 - acc: 0.9432 - val_loss: 0.6731 - val_acc: 0.8213\n",
      "Epoch 315/400\n",
      "88/88 [==============================] - 0s 842us/step - loss: 0.1496 - acc: 0.9432 - val_loss: 0.6329 - val_acc: 0.8309\n",
      "Epoch 316/400\n",
      "88/88 [==============================] - 0s 807us/step - loss: 0.1516 - acc: 0.9470 - val_loss: 0.6733 - val_acc: 0.8309\n",
      "Epoch 317/400\n",
      "88/88 [==============================] - 0s 788us/step - loss: 0.1180 - acc: 0.9621 - val_loss: 0.7083 - val_acc: 0.8406\n",
      "Epoch 318/400\n",
      "88/88 [==============================] - 0s 778us/step - loss: 0.1564 - acc: 0.9356 - val_loss: 0.6760 - val_acc: 0.8357\n",
      "Epoch 319/400\n",
      "88/88 [==============================] - 0s 858us/step - loss: 0.1329 - acc: 0.9495 - val_loss: 0.6143 - val_acc: 0.8213\n",
      "Epoch 320/400\n",
      "88/88 [==============================] - 0s 861us/step - loss: 0.1410 - acc: 0.9457 - val_loss: 0.6612 - val_acc: 0.8309\n",
      "Epoch 321/400\n",
      "88/88 [==============================] - 0s 893us/step - loss: 0.1167 - acc: 0.9520 - val_loss: 0.6682 - val_acc: 0.8357\n",
      "Epoch 322/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1362 - acc: 0.9419 - val_loss: 0.6349 - val_acc: 0.8357\n",
      "Epoch 323/400\n",
      "88/88 [==============================] - 0s 937us/step - loss: 0.1280 - acc: 0.9482 - val_loss: 0.5984 - val_acc: 0.8164\n",
      "Epoch 324/400\n",
      "88/88 [==============================] - 0s 958us/step - loss: 0.1394 - acc: 0.9470 - val_loss: 0.6659 - val_acc: 0.8261\n",
      "Epoch 325/400\n",
      "88/88 [==============================] - 0s 815us/step - loss: 0.1274 - acc: 0.9545 - val_loss: 0.6778 - val_acc: 0.8261\n",
      "Epoch 326/400\n",
      "88/88 [==============================] - 0s 891us/step - loss: 0.1277 - acc: 0.9482 - val_loss: 0.6418 - val_acc: 0.8357\n",
      "Epoch 327/400\n",
      "88/88 [==============================] - 0s 867us/step - loss: 0.1146 - acc: 0.9571 - val_loss: 0.6161 - val_acc: 0.8213\n",
      "Epoch 328/400\n",
      "88/88 [==============================] - 0s 895us/step - loss: 0.1306 - acc: 0.9495 - val_loss: 0.6803 - val_acc: 0.8406\n",
      "Epoch 329/400\n",
      "88/88 [==============================] - 0s 880us/step - loss: 0.1305 - acc: 0.9470 - val_loss: 0.6789 - val_acc: 0.8502\n",
      "Epoch 330/400\n",
      "88/88 [==============================] - 0s 828us/step - loss: 0.1203 - acc: 0.9545 - val_loss: 0.6241 - val_acc: 0.8357\n",
      "Epoch 331/400\n",
      "88/88 [==============================] - 0s 961us/step - loss: 0.1246 - acc: 0.9545 - val_loss: 0.6348 - val_acc: 0.8309\n",
      "Epoch 332/400\n",
      "88/88 [==============================] - 0s 826us/step - loss: 0.1299 - acc: 0.9482 - val_loss: 0.6846 - val_acc: 0.8454\n",
      "Epoch 333/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1175 - acc: 0.9508 - val_loss: 0.6981 - val_acc: 0.8309\n",
      "Epoch 334/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1284 - acc: 0.9571 - val_loss: 0.6892 - val_acc: 0.8357\n",
      "Epoch 335/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1164 - acc: 0.9596 - val_loss: 0.6873 - val_acc: 0.8406\n",
      "Epoch 336/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1301 - acc: 0.9432 - val_loss: 0.6593 - val_acc: 0.8261\n",
      "Epoch 337/400\n",
      "88/88 [==============================] - 0s 956us/step - loss: 0.1148 - acc: 0.9571 - val_loss: 0.6918 - val_acc: 0.8357\n",
      "Epoch 338/400\n",
      "88/88 [==============================] - 0s 999us/step - loss: 0.1196 - acc: 0.9571 - val_loss: 0.6947 - val_acc: 0.8406\n",
      "Epoch 339/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1289 - acc: 0.9520 - val_loss: 0.6335 - val_acc: 0.8357\n",
      "Epoch 340/400\n",
      "88/88 [==============================] - 0s 939us/step - loss: 0.1111 - acc: 0.9609 - val_loss: 0.6021 - val_acc: 0.8309\n",
      "Epoch 341/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1442 - acc: 0.9407 - val_loss: 0.6794 - val_acc: 0.8406\n",
      "Epoch 342/400\n",
      "88/88 [==============================] - 0s 957us/step - loss: 0.1141 - acc: 0.9533 - val_loss: 0.6885 - val_acc: 0.8164\n",
      "Epoch 343/400\n",
      "88/88 [==============================] - 0s 886us/step - loss: 0.1248 - acc: 0.9482 - val_loss: 0.6243 - val_acc: 0.8213\n",
      "Epoch 344/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1482 - acc: 0.9343 - val_loss: 0.6917 - val_acc: 0.8213\n",
      "Epoch 345/400\n",
      "88/88 [==============================] - 0s 952us/step - loss: 0.1299 - acc: 0.9470 - val_loss: 0.7026 - val_acc: 0.8406\n",
      "Epoch 346/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1249 - acc: 0.9520 - val_loss: 0.6809 - val_acc: 0.8068\n",
      "Epoch 347/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1346 - acc: 0.9533 - val_loss: 0.6833 - val_acc: 0.8261\n",
      "Epoch 348/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1110 - acc: 0.9609 - val_loss: 0.6804 - val_acc: 0.8406\n",
      "Epoch 349/400\n",
      "88/88 [==============================] - 0s 924us/step - loss: 0.1077 - acc: 0.9609 - val_loss: 0.6618 - val_acc: 0.8261\n",
      "Epoch 350/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1280 - acc: 0.9495 - val_loss: 0.6225 - val_acc: 0.8309\n",
      "Epoch 351/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1333 - acc: 0.9457 - val_loss: 0.6517 - val_acc: 0.8261\n",
      "Epoch 352/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1324 - acc: 0.9470 - val_loss: 0.6666 - val_acc: 0.8454\n",
      "Epoch 353/400\n",
      "88/88 [==============================] - 0s 933us/step - loss: 0.1169 - acc: 0.9596 - val_loss: 0.6626 - val_acc: 0.8213\n",
      "Epoch 354/400\n",
      "88/88 [==============================] - 0s 812us/step - loss: 0.1263 - acc: 0.9508 - val_loss: 0.5974 - val_acc: 0.8357\n",
      "Epoch 355/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1298 - acc: 0.9444 - val_loss: 0.7022 - val_acc: 0.8213\n",
      "Epoch 356/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1631 - acc: 0.9306 - val_loss: 0.7014 - val_acc: 0.8261\n",
      "Epoch 357/400\n",
      "88/88 [==============================] - 0s 972us/step - loss: 0.1251 - acc: 0.9508 - val_loss: 0.6290 - val_acc: 0.8261\n",
      "Epoch 358/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1559 - acc: 0.9356 - val_loss: 0.7012 - val_acc: 0.8357\n",
      "Epoch 359/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1203 - acc: 0.9609 - val_loss: 0.7110 - val_acc: 0.8261\n",
      "Epoch 360/400\n",
      "88/88 [==============================] - 0s 861us/step - loss: 0.1246 - acc: 0.9571 - val_loss: 0.6433 - val_acc: 0.8213\n",
      "Epoch 361/400\n",
      "88/88 [==============================] - 0s 987us/step - loss: 0.1437 - acc: 0.9419 - val_loss: 0.6727 - val_acc: 0.8357\n",
      "Epoch 362/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1194 - acc: 0.9596 - val_loss: 0.7607 - val_acc: 0.8164\n",
      "Epoch 363/400\n",
      "88/88 [==============================] - 0s 819us/step - loss: 0.1751 - acc: 0.9306 - val_loss: 0.6470 - val_acc: 0.8357\n",
      "Epoch 364/400\n",
      "88/88 [==============================] - 0s 1000us/step - loss: 0.1176 - acc: 0.9545 - val_loss: 0.6417 - val_acc: 0.8261\n",
      "Epoch 365/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1361 - acc: 0.9482 - val_loss: 0.6973 - val_acc: 0.8357\n",
      "Epoch 366/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1272 - acc: 0.9520 - val_loss: 0.6703 - val_acc: 0.8357\n",
      "Epoch 367/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1257 - acc: 0.9495 - val_loss: 0.6084 - val_acc: 0.8454\n",
      "Epoch 368/400\n",
      "88/88 [==============================] - 0s 798us/step - loss: 0.1081 - acc: 0.9609 - val_loss: 0.6309 - val_acc: 0.8454\n",
      "Epoch 369/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1150 - acc: 0.9533 - val_loss: 0.7004 - val_acc: 0.8309\n",
      "Epoch 370/400\n",
      "88/88 [==============================] - 0s 923us/step - loss: 0.1176 - acc: 0.9545 - val_loss: 0.7110 - val_acc: 0.8261\n",
      "Epoch 371/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1082 - acc: 0.9672 - val_loss: 0.6212 - val_acc: 0.8116\n",
      "Epoch 372/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 0s 921us/step - loss: 0.1269 - acc: 0.9520 - val_loss: 0.6757 - val_acc: 0.8261\n",
      "Epoch 373/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1126 - acc: 0.9545 - val_loss: 0.7286 - val_acc: 0.8261\n",
      "Epoch 374/400\n",
      "88/88 [==============================] - 0s 939us/step - loss: 0.1176 - acc: 0.9558 - val_loss: 0.7158 - val_acc: 0.8309\n",
      "Epoch 375/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1116 - acc: 0.9533 - val_loss: 0.6593 - val_acc: 0.8164\n",
      "Epoch 376/400\n",
      "88/88 [==============================] - 0s 908us/step - loss: 0.1152 - acc: 0.9533 - val_loss: 0.7091 - val_acc: 0.8261\n",
      "Epoch 377/400\n",
      "88/88 [==============================] - 0s 980us/step - loss: 0.1097 - acc: 0.9558 - val_loss: 0.7059 - val_acc: 0.8357\n",
      "Epoch 378/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1123 - acc: 0.9520 - val_loss: 0.6869 - val_acc: 0.8261\n",
      "Epoch 379/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1129 - acc: 0.9596 - val_loss: 0.7070 - val_acc: 0.8261\n",
      "Epoch 380/400\n",
      "88/88 [==============================] - 0s 945us/step - loss: 0.1035 - acc: 0.9621 - val_loss: 0.6971 - val_acc: 0.8357\n",
      "Epoch 381/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1118 - acc: 0.9596 - val_loss: 0.6968 - val_acc: 0.8309\n",
      "Epoch 382/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1071 - acc: 0.9609 - val_loss: 0.7090 - val_acc: 0.8309\n",
      "Epoch 383/400\n",
      "88/88 [==============================] - 0s 988us/step - loss: 0.1121 - acc: 0.9520 - val_loss: 0.6960 - val_acc: 0.8309\n",
      "Epoch 384/400\n",
      "88/88 [==============================] - 0s 929us/step - loss: 0.1045 - acc: 0.9596 - val_loss: 0.7192 - val_acc: 0.8261\n",
      "Epoch 385/400\n",
      "88/88 [==============================] - 0s 973us/step - loss: 0.1202 - acc: 0.9508 - val_loss: 0.6629 - val_acc: 0.8261\n",
      "Epoch 386/400\n",
      "88/88 [==============================] - 0s 969us/step - loss: 0.0894 - acc: 0.9785 - val_loss: 0.6685 - val_acc: 0.8406\n",
      "Epoch 387/400\n",
      "88/88 [==============================] - 0s 969us/step - loss: 0.1023 - acc: 0.9634 - val_loss: 0.7031 - val_acc: 0.8357\n",
      "Epoch 388/400\n",
      "88/88 [==============================] - 0s 991us/step - loss: 0.0955 - acc: 0.9596 - val_loss: 0.6923 - val_acc: 0.8406\n",
      "Epoch 389/400\n",
      "88/88 [==============================] - 0s 897us/step - loss: 0.1107 - acc: 0.9545 - val_loss: 0.6561 - val_acc: 0.8309\n",
      "Epoch 390/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1034 - acc: 0.9659 - val_loss: 0.7147 - val_acc: 0.8213\n",
      "Epoch 391/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.0885 - acc: 0.9760 - val_loss: 0.7498 - val_acc: 0.8309\n",
      "Epoch 392/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1023 - acc: 0.9646 - val_loss: 0.7309 - val_acc: 0.8213\n",
      "Epoch 393/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1084 - acc: 0.9609 - val_loss: 0.7295 - val_acc: 0.8068\n",
      "Epoch 394/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.0936 - acc: 0.9684 - val_loss: 0.7380 - val_acc: 0.8309\n",
      "Epoch 395/400\n",
      "88/88 [==============================] - 0s 846us/step - loss: 0.1083 - acc: 0.9609 - val_loss: 0.7494 - val_acc: 0.8116\n",
      "Epoch 396/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1056 - acc: 0.9684 - val_loss: 0.7299 - val_acc: 0.8213\n",
      "Epoch 397/400\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1101 - acc: 0.9621 - val_loss: 0.7136 - val_acc: 0.8116\n",
      "Epoch 398/400\n",
      "88/88 [==============================] - 0s 897us/step - loss: 0.0983 - acc: 0.9697 - val_loss: 0.6700 - val_acc: 0.8309\n",
      "Epoch 399/400\n",
      "88/88 [==============================] - 0s 928us/step - loss: 0.1357 - acc: 0.9457 - val_loss: 0.6753 - val_acc: 0.8213\n",
      "Epoch 400/400\n",
      "88/88 [==============================] - 0s 997us/step - loss: 0.0971 - acc: 0.9634 - val_loss: 0.6988 - val_acc: 0.8164\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f69541e2ef0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.fit(lstm_in, y, epochs=400, batch_size=128, shuffle=True, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
